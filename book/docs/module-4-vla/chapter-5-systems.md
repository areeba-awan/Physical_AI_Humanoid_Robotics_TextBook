---
sidebar_position: 5
title: "4.5 Ø§ÛŒÙ†Úˆ Ù¹Ùˆ Ø§ÛŒÙ†Úˆ VLA Ø³Ø³Ù¹Ù…Ø²"
description: Ø¢ÙˆØ§Ø² Ú©Ù†Ù¹Ø±ÙˆÙ„ Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ú©Ù…Ù„ VLA Ø±ÙˆØ¨ÙˆÙ¹ Ø³Ø³Ù¹Ù…Ø² Ø¨Ù†Ø§Ù†Ø§
keywords: [VLA, Ø§ÛŒÙ†Úˆ Ù¹Ùˆ Ø§ÛŒÙ†Úˆ, Ø³Ø³Ù¹Ù…, ÚˆÛŒÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹, Ø¢ÙˆØ§Ø² Ú©Ù†Ù¹Ø±ÙˆÙ„, Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„]
---

# Ø¨Ø§Ø¨ 4.5: Ø§ÛŒÙ†Úˆ Ù¹Ùˆ Ø§ÛŒÙ†Úˆ VLA Ø³Ø³Ù¹Ù…Ø²

## Ø³ÛŒÚ©Ú¾Ù†Û’ Ú©Û’ Ù…Ù‚Ø§ØµØ¯

- VLA Ø§Ø¬Ø²Ø§Ø¡ Ú©Ùˆ Ù…Ú©Ù…Ù„ Ø³Ø³Ù¹Ù… Ù…ÛŒÚº ÛŒÚ©Ø¬Ø§ Ú©Ø±ÛŒÚº
- VLA Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ Ø±ÙˆØ¨ÙˆÙ¹ ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ù¾Ø± ÚˆÛŒÙ¾Ù„Ø§Ø¦ÛŒ Ú©Ø±ÛŒÚº
- Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©Û’ Ú†ÛŒÙ„Ù†Ø¬Ø² Ø³Ù†Ø¨Ú¾Ø§Ù„ÛŒÚº
- Ø§Ù†ÙØ±Ù†Ø³ Ù¾Ø±ÙØ§Ø±Ù…Ù†Ø³ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø² Ú©Ø±ÛŒÚº
- **Ø¢ÙˆØ§Ø² Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Û’ VLA Ø³Ø³Ù¹Ù…Ø² Ø¨Ù†Ø§Ø¦ÛŒÚº**
- **Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ Ø§Ù†Ù¹Ø±ÛŒÚ©Ø´Ù† Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù†Ø² Ù„Ø§Ú¯Ùˆ Ú©Ø±ÛŒÚº**

## Ù…Ú©Ù…Ù„ VLA Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Ø¢ÙˆØ§Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ø§ÛŒÙ†Úˆ Ù¹Ùˆ Ø§ÛŒÙ†Úˆ VLA Ø³Ø³Ù¹Ù…               â”‚
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ Ú©ÛŒÙ…Ø±Û   â”‚â”€â”€â”                      â”Œâ”€â”€â”€>â”‚ Ø±ÙˆØ¨ÙˆÙ¹   â”‚      â”‚
â”‚   â”‚ Ø³Ù¹Ø±ÛŒÙ…  â”‚  â”‚                      â”‚    â”‚ Ú©Ù†Ù¹Ø±ÙˆÙ„  â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                â”‚    â”‚            â”‚    â”‚                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”œâ”€â”€â”€>â”‚    VLA     â”‚â”€â”€â”€â”€â”¤    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ Ú©Ù„Ø§Ø¦ÛŒ   â”‚â”€â”€â”¤    â”‚   Ù…Ø§ÚˆÙ„    â”‚    â”‚    â”‚ Ú¯Ø±Ù¾Ø±   â”‚      â”‚
â”‚   â”‚ Ú©ÛŒÙ…Ø±Û   â”‚  â”‚    â”‚            â”‚    â””â”€â”€â”€>â”‚ Ú©Ù†Ù¹Ø±ÙˆÙ„  â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                â”‚          â–²                      â”‚           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          â”‚                      â”‚           â”‚
â”‚   â”‚ğŸ¤ Ø¢ÙˆØ§Ø²  â”‚â”€â”€â”˜          â”‚                      â–¼           â”‚
â”‚   â”‚ Ø§Ù† Ù¾Ù¹   â”‚         ÙÛŒÚˆØ¨ÛŒÚ©            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚               â”‚ğŸ”Š Ø¢ÙˆØ§Ø²  â”‚       â”‚
â”‚        â–²                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Ø¢Ø¤Ù¹ Ù¾Ù¹ â”‚       â”‚
â”‚        â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚   [ÙˆÛŒÚ© ÙˆØ±Úˆ: "Ø§Ø±Û’ Ø±ÙˆØ¨ÙˆÙ¹"]                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Ø³Ø³Ù¹Ù… Ú©Ø§ Ù†ÙØ§Ø°

```python
class VLASystem:
    def __init__(self, model_path, robot_interface):
        self.model = OpenVLA.from_pretrained(model_path)
        self.robot = robot_interface
        self.cameras = CameraArray()

    def run(self, instruction):
        """ÛØ¯Ø§ÛŒØ§Øª Ú©Ùˆ Ø§ÛŒÙ†Úˆ Ù¹Ùˆ Ø§ÛŒÙ†Úˆ Ø¹Ù…Ù„ Ù…ÛŒÚº Ù„Ø§Ø¦ÛŒÚº"""
        while not self.task_complete():
            # Ù…ÙˆØ¬ÙˆØ¯Û Ù…Ø´Ø§ÛØ¯Û Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº
            images = self.cameras.capture()

            # Ø§ÛŒÚ©Ø´Ù† Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ú©Ø±ÛŒÚº
            action = self.model.predict(
                images=images,
                instruction=instruction
            )

            # Ø§ÛŒÚ©Ø´Ù† Ø¹Ù…Ù„ Ù…ÛŒÚº Ù„Ø§Ø¦ÛŒÚº
            self.robot.execute(action)

            # ØªÚ©Ù…ÛŒÙ„ Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº
            if self.detect_goal_reached(images, instruction):
                break

        return True
```

## Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©Û’ Ú†ÛŒÙ„Ù†Ø¬Ø²

### 1. Ù„ÛŒÙ¹Ù†Ø³ÛŒ Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹

```python
class LatencyOptimizer:
    def __init__(self, target_hz=10):
        self.target_period = 1.0 / target_hz
        self.action_buffer = deque(maxlen=5)

    def run_loop(self, vla_system):
        while True:
            start = time.time()

            # Ø¨ÙØ±Úˆ Ø§ÛŒÚ©Ø´Ù† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº ÛŒØ§ Ù†ÛŒØ§ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ú©Ø±ÛŒÚº
            if self.action_buffer:
                action = self.action_buffer.popleft()
            else:
                action = vla_system.predict()
                # Ø§Ø¶Ø§ÙÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒØ§Úº Ø¨ÙØ± Ú©Ø±ÛŒÚº
                self.prefetch_actions(vla_system)

            vla_system.execute(action)

            # Ú©Ù†Ù¹Ø±ÙˆÙ„ ÙØ±ÛŒÚ©ÙˆØ¦Ù†Ø³ÛŒ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ÛŒÚº
            elapsed = time.time() - start
            if elapsed < self.target_period:
                time.sleep(self.target_period - elapsed)
```

### 2. Ù†Ø§Ú©Ø§Ù…ÛŒ Ú©ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ

```python
class FailureRecovery:
    def __init__(self):
        self.failure_detectors = [
            GraspFailureDetector(),     # Ú¯Ø±Ù¾ Ù†Ø§Ú©Ø§Ù…ÛŒ Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§Ù†Û’ ÙˆØ§Ù„Ø§
            CollisionDetector(),         # Ù¹Ú©Ø±Ø§Ø¤ Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§Ù†Û’ ÙˆØ§Ù„Ø§
            StuckDetector(),             # Ù¾Ú¾Ù†Ø³Ù†Û’ Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§Ù†Û’ ÙˆØ§Ù„Ø§
        ]

    def monitor_and_recover(self, vla_system):
        for detector in self.failure_detectors:
            if detector.detect():
                recovery_action = detector.get_recovery()
                vla_system.execute(recovery_action)
                return True
        return False
```

### 3. Ø­ÙØ§Ø¸ØªÛŒ Ù¾Ø§Ø¨Ù†Ø¯ÛŒØ§Úº

```python
class SafetyFilter:
    def __init__(self, workspace_bounds, max_velocity):
        self.bounds = workspace_bounds
        self.max_vel = max_velocity

    def filter_action(self, action, current_state):
        # ÙˆØ±Ú© Ø§Ø³Ù¾ÛŒØ³ Ù…ÛŒÚº Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±ÛŒÚº
        target_pos = current_state.position + action.delta_position
        target_pos = np.clip(target_pos, self.bounds[0], self.bounds[1])

        # Ø±ÙØªØ§Ø± Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±ÛŒÚº
        action.delta_position = np.clip(
            action.delta_position,
            -self.max_vel,
            self.max_vel
        )

        return action
```

## ÚˆÛŒÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†

### TensorRT Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†

```python
import tensorrt as trt

def optimize_model(model_path, output_path):
    """ØªÛŒØ² Ø§Ù†ÙØ±Ù†Ø³ Ú©Û’ Ù„ÛŒÛ’ TensorRT Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ÛŒÚº"""
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network()

    # ONNX Ù…Ø§ÚˆÙ„ Ù¾Ø§Ø±Ø³ Ú©Ø±ÛŒÚº
    parser = trt.OnnxParser(network, logger)
    with open(model_path, 'rb') as f:
        parser.parse(f.read())

    # Ø§Ù†Ø¬Ù† Ø¨Ù†Ø§Ø¦ÛŒÚº
    config = builder.create_builder_config()
    config.set_flag(trt.BuilderFlag.FP16)  # FP16 Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº

    engine = builder.build_engine(network, config)

    with open(output_path, 'wb') as f:
        f.write(engine.serialize())
```

### Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù†

```python
from transformers import AutoModelForCausalLM
import torch

# Ù…Ø§ÚˆÙ„ Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº
model = AutoModelForCausalLM.from_pretrained("openvla/openvla-7b")

# 4-Ø¨Ù¹ Ù…ÛŒÚº Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø² Ú©Ø±ÛŒÚº
model = model.to(torch.bfloat16)
model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

---

## Ø¢ÙˆØ§Ø² Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Ø§ VLA Ø³Ø³Ù¹Ù…

Ø¢ÙˆØ§Ø² Ú©Ù†Ù¹Ø±ÙˆÙ„ Ú©Ùˆ ÛŒÚ©Ø¬Ø§ Ú©Ø±Ù†Ø§ Ø±ÙˆØ¨ÙˆÙ¹ Ø¢Ù¾Ø±ÛŒØ´Ù† Ú©Û’ Ù„ÛŒÛ’ Ù‚Ø¯Ø±ØªÛŒØŒ ÛÛŒÙ†ÚˆØ² ÙØ±ÛŒ Ø§Ù†Ù¹Ø±ÙÛŒØ³ Ø¨Ù†Ø§ØªØ§ ÛÛ’Û”

### Ù…Ú©Ù…Ù„ Ø¢ÙˆØ§Ø²-VLA Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Ø¢ÙˆØ§Ø² Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Ø§ VLA Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±                    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Ø§Ù† Ù¾Ù¹ Ù¾Ø±Øª                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  RGB   â”‚  â”‚ ÚˆÛŒÙ¾ØªÚ¾  â”‚  â”‚ Ú©Ù„Ø§Ø¦ÛŒ  â”‚  â”‚   Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆÙÙˆÙ†   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ Ú©ÛŒÙ…Ø±Û  â”‚  â”‚ Ú©ÛŒÙ…Ø±Û  â”‚  â”‚ Ú©ÛŒÙ…Ø±Û  â”‚  â”‚   (16kHz)      â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚          â”‚          â”‚                â”‚               â”‚
â”‚         â–¼          â–¼          â–¼                â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ù¾Ø±Øª                             â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚   ÙˆÛŒÚ˜Ù† Ø§Ù†Ú©ÙˆÚˆØ±      â”‚    â”‚    Whisper STT          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚       (ViT)         â”‚    â”‚    + ÙˆÛŒÚ© ÙˆØ±Úˆ           â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚             â”‚                           â”‚                 â”‚  â”‚
â”‚  â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚  â”‚
â”‚  â”‚                         â–¼                                 â”‚  â”‚
â”‚  â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚  â”‚
â”‚  â”‚              â”‚   VLA ÙÛŒÙˆÚ˜Ù† Ù…Ø§ÚˆÙ„    â”‚                    â”‚  â”‚
â”‚  â”‚              â”‚  (ÙˆÛŒÚ˜Ù† + Ø²Ø¨Ø§Ù†)      â”‚                    â”‚  â”‚
â”‚  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚
â”‚  â”‚                         â”‚                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   Ø¢Ø¤Ù¹ Ù¾Ù¹ Ù¾Ø±Øª                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚   Ø§ÛŒÚ©Ø´Ù†   â”‚  â”‚  Ø­ÙØ§Ø¸ØªÛŒ   â”‚  â”‚   Ø¢ÙˆØ§Ø² ÙÛŒÚˆØ¨ÛŒÚ©     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  ÚˆÛŒÚ©ÙˆÚˆØ±  â”‚â”€â”€â”‚   ÙÙ„Ù¹Ø±    â”‚â”€â”€â”‚   (TTS Ø¬ÙˆØ§Ø¨)      â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ø¢ÙˆØ§Ø²-VLA Ø³Ø³Ù¹Ù… Ú©Ø§ Ù†ÙØ§Ø°

```python
import whisper
import numpy as np
import sounddevice as sd
from gtts import gTTS
import threading
import queue

class VoiceEnabledVLA:
    """Ø¢ÙˆØ§Ø² Ú©Ù†Ù¹Ø±ÙˆÙ„ Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ú©Ù…Ù„ VLA Ø³Ø³Ù¹Ù…"""

    def __init__(self, model_path, robot_interface):
        # VLA Ø§Ø¬Ø²Ø§Ø¡
        self.vla_model = OpenVLA.from_pretrained(model_path)
        self.robot = robot_interface
        self.cameras = CameraArray()

        # Ø¢ÙˆØ§Ø² Ú©Û’ Ø§Ø¬Ø²Ø§Ø¡
        self.whisper = whisper.load_model("small")
        self.audio_queue = queue.Queue()
        self.is_listening = True

        # Ø­Ø§Ù„Øª
        self.current_task = None
        self.task_active = False

        # Ø­ÙØ§Ø¸Øª
        self.safety_filter = SafetyFilter(
            workspace_bounds=[[-0.5, -0.5, 0], [0.5, 0.5, 0.5]],
            max_velocity=0.1
        )

    def start(self):
        """Ø¢ÙˆØ§Ø² Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Ø§ VLA Ø³Ø³Ù¹Ù… Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº"""
        print("Ø¢ÙˆØ§Ø² Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Ø§ VLA Ø³Ø³Ù¹Ù… Ø´Ø±ÙˆØ¹ ÛÙˆ Ø±ÛØ§ ÛÛ’...")
        print("'Ø§Ø±Û’ Ø±ÙˆØ¨ÙˆÙ¹' Ú©ÛÛŒÚº Ø§ÙˆØ± Ù¾Ú¾Ø± Ø§Ù¾Ù†Ø§ Ø­Ú©Ù… Ø¯ÛŒÚº")

        # Ø¢ÚˆÛŒÙˆ Ø³Ù†Ù†Û’ ÙˆØ§Ù„Ø§ ØªÚ¾Ø±ÛŒÚˆ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº
        self.audio_thread = threading.Thread(target=self._audio_listener)
        self.audio_thread.start()

        # Ù…ÛŒÙ† Ú©Ù†Ù¹Ø±ÙˆÙ„ Ù„ÙˆÙ¾ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº
        self._control_loop()

    def _audio_listener(self):
        """Ù…Ø³Ù„Ø³Ù„ Ø¢ÙˆØ§Ø² Ú©Û’ Ø§Ø­Ú©Ø§Ù…Ø§Øª Ø³Ù†ÛŒÚº"""
        with sd.InputStream(samplerate=16000, channels=1, dtype=np.int16) as stream:
            buffer = []
            silence_count = 0

            while self.is_listening:
                audio_chunk, _ = stream.read(1024)
                buffer.extend(audio_chunk.flatten())

                # Ø¢ÙˆØ§Ø² Ú©ÛŒ Ø³Ø±Ú¯Ø±Ù…ÛŒ Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§Ø¦ÛŒÚº
                if np.abs(audio_chunk).mean() > 500:
                    silence_count = 0
                else:
                    silence_count += 1

                # 1 Ø³ÛŒÚ©Ù†Úˆ Ø®Ø§Ù…ÙˆØ´ÛŒ Ú©Û’ Ø¨Ø¹Ø¯ Ù¾Ø±ÙˆØ³ÛŒØ³ Ú©Ø±ÛŒÚº
                if silence_count > 15 and len(buffer) > 16000:
                    audio = np.array(buffer, dtype=np.float32) / 32768.0
                    self.audio_queue.put(audio)
                    buffer = []
                    silence_count = 0

    def _control_loop(self):
        """Ø¢ÙˆØ§Ø² Ø§ÙˆØ± VLA Ú©Ùˆ Ø³Ù†Ø¨Ú¾Ø§Ù„Ù†Û’ ÙˆØ§Ù„Ø§ Ù…ÛŒÙ† Ú©Ù†Ù¹Ø±ÙˆÙ„ Ù„ÙˆÙ¾"""
        while True:
            # Ù†Ø¦Û’ Ø¢ÙˆØ§Ø² Ú©Û’ Ø­Ú©Ù… Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº
            try:
                audio = self.audio_queue.get_nowait()
                command = self._process_voice(audio)

                if command:
                    self._handle_voice_command(command)

            except queue.Empty:
                pass

            # ÙØ¹Ø§Ù„ Ú©Ø§Ù… Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒÚº
            if self.task_active and self.current_task:
                self._execute_vla_step()

    def _process_voice(self, audio):
        """Ø¢ÚˆÛŒÙˆ Ú©Ùˆ Ù¹ÛŒÚ©Ø³Ù¹ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ÛŒÚº"""
        result = self.whisper.transcribe(audio, language="ur")
        text = result["text"].strip().lower()

        # ÙˆÛŒÚ© ÙˆØ±Úˆ Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº
        if "Ø§Ø±Û’ Ø±ÙˆØ¨ÙˆÙ¹" in text or "hey robot" in text:
            # ÙˆÛŒÚ© ÙˆØ±Úˆ Ú©Û’ Ø¨Ø¹Ø¯ Ø­Ú©Ù… Ù†Ú©Ø§Ù„ÛŒÚº
            if "Ø§Ø±Û’ Ø±ÙˆØ¨ÙˆÙ¹" in text:
                command = text.split("Ø§Ø±Û’ Ø±ÙˆØ¨ÙˆÙ¹")[-1].strip()
            else:
                command = text.split("hey robot")[-1].strip()
            return command

        return None

    def _handle_voice_command(self, command):
        """Ø¢ÙˆØ§Ø² Ú©Ø§ Ø­Ú©Ù… Ù¾Ø±ÙˆØ³ÛŒØ³ Ú©Ø±ÛŒÚº"""
        print(f"Ø­Ú©Ù… Ù…ÙˆØµÙˆÙ„ ÛÙˆØ§: {command}")

        # Ú©Ù†Ù¹Ø±ÙˆÙ„ Ú©Ù…Ø§Ù†ÚˆØ² Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº
        if "Ø±Ú©Ùˆ" in command or "stop" in command:
            self.task_active = False
            self.robot.stop()
            self._speak("Ø±Ú© Ø±ÛØ§ ÛÙˆÚº")
            return

        if "ÙˆÙ‚ÙÛ" in command or "pause" in command:
            self.task_active = False
            self._speak("Ø±Ú©Ø§ ÛÙˆØ§")
            return

        if "Ø¬Ø§Ø±ÛŒ" in command or "resume" in command:
            self.task_active = True
            self._speak("Ø¬Ø§Ø±ÛŒ ÛÛ’")
            return

        # Ù†ÛŒØ§ Ù¹Ø§Ø³Ú© Ú©Ù…Ø§Ù†Úˆ
        self.current_task = command
        self.task_active = True
        self._speak(f"Ú©Ø§Ù… Ø´Ø±ÙˆØ¹: {command}")

    def _execute_vla_step(self):
        """VLA Ú©Ù†Ù¹Ø±ÙˆÙ„ Ú©Ø§ Ø§ÛŒÚ© Ù…Ø±Ø­Ù„Û Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒÚº"""
        # Ù…Ø´Ø§ÛØ¯Û Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº
        images = self.cameras.capture()

        # Ø§ÛŒÚ©Ø´Ù† Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ú©Ø±ÛŒÚº
        action = self.vla_model.predict(
            images=images,
            instruction=self.current_task
        )

        # Ø­ÙØ§Ø¸ØªÛŒ ÙÙ„Ù¹Ø± Ù„Ú¯Ø§Ø¦ÛŒÚº
        current_state = self.robot.get_state()
        safe_action = self.safety_filter.filter_action(action, current_state)

        # Ø§ÛŒÚ©Ø´Ù† Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒÚº
        self.robot.execute(safe_action)

        # ØªÚ©Ù…ÛŒÙ„ Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº
        if self._detect_task_complete(images):
            self.task_active = False
            self._speak("Ú©Ø§Ù… Ù…Ú©Ù…Ù„!")

    def _detect_task_complete(self, images):
        """Ø¬Ø§Ù†Ú†ÛŒÚº Ú©Û Ù…ÙˆØ¬ÙˆØ¯Û Ú©Ø§Ù… Ù…Ú©Ù…Ù„ ÛÛ’"""
        # VLA Ù…Ø§ÚˆÙ„ ÛŒØ§ Ø¹Ù„ÛŒØ­Ø¯Û ØªÚ©Ù…ÛŒÙ„ ÚˆÛŒÙ¹ÛŒÚ©Ù¹Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
        completion_score = self.vla_model.check_completion(
            images=images,
            instruction=self.current_task
        )
        return completion_score > 0.9

    def _speak(self, text):
        """Ù¹ÛŒÚ©Ø³Ù¹ Ø³Û’ Ø¢ÙˆØ§Ø² Ø¢Ø¤Ù¹ Ù¾Ù¹"""
        tts = gTTS(text=text, lang='ur')
        tts.save("/tmp/response.mp3")
        # Ø¢ÚˆÛŒÙˆ Ú†Ù„Ø§Ø¦ÛŒÚº (Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… Ù¾Ø± Ù…Ù†Ø­ØµØ±)
        import pygame
        pygame.mixer.init()
        pygame.mixer.music.load("/tmp/response.mp3")
        pygame.mixer.music.play()

    def stop(self):
        """Ø³Ø³Ù¹Ù… Ø¨Ù†Ø¯ Ú©Ø±ÛŒÚº"""
        self.is_listening = False
        self.task_active = False
        self.robot.stop()
```

### Ú¯ÙØªÚ¯Ùˆ Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ø±ÙˆØ¨ÙˆÙ¹ Ú©Ù†Ù¹Ø±ÙˆÙ„

Ù¾ÛŒÚ†ÛŒØ¯Û Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ù‚Ø¯Ø±ØªÛŒ Ú¯ÙØªÚ¯Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±ÛŒÚº:

```python
from openai import OpenAI

class ConversationalVLA:
    """Ú¯ÙØªÚ¯Ùˆ Ø§Ù†Ù¹Ø±ÙÛŒØ³ Ú©Û’ Ø³Ø§ØªÚ¾ VLA"""

    def __init__(self, vla_system):
        self.vla = vla_system
        self.llm = OpenAI()
        self.conversation_history = []
        self.scene_context = {}

    def process_utterance(self, user_input, current_image):
        """Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Û’ Ø³Ø§ØªÚ¾ Ù‚Ø¯Ø±ØªÛŒ Ø²Ø¨Ø§Ù† Ù¾Ø±ÙˆØ³ÛŒØ³ Ú©Ø±ÛŒÚº"""

        # Ø³ÛŒÙ† Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ø§Ù¾ÚˆÛŒÙ¹ Ú©Ø±ÛŒÚº
        self.scene_context = self._analyze_scene(current_image)

        # Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Û’ Ø³Ø§ØªÚ¾ Ù¾Ø±Ø§Ù…Ù¾Ù¹ Ø¨Ù†Ø§Ø¦ÛŒÚº
        system_prompt = f"""Ø¢Ù¾ Ø§ÛŒÚ© Ù…Ø¯Ø¯Ú¯Ø§Ø± Ø±ÙˆØ¨ÙˆÙ¹ Ø§Ø³Ø³Ù¹Ù†Ù¹ ÛÛŒÚºÛ”

Ù…ÙˆØ¬ÙˆØ¯Û Ø³ÛŒÙ†: {self.scene_context}
Ø±ÙˆØ¨ÙˆÙ¹ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº: Ø§Ù¹Ú¾Ø§Ù†Ø§ØŒ Ø±Ú©Ú¾Ù†Ø§ØŒ ÛÙ„Ù†Ø§ØŒ ÚˆØ§Ù„Ù†Ø§ØŒ Ú©Ú¾ÙˆÙ„Ù†Ø§ØŒ Ø¨Ù†Ø¯ Ú©Ø±Ù†Ø§

ØµØ§Ø±Ù Ú©ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø±:
1. Ø±ÙˆØ¨ÙˆÙ¹ Ø§ÛŒÚ©Ø´Ù† Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒÚº (ACTION: <command> Ø³Û’ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚº)
2. ÙˆØ¶Ø§Ø­Øª Ù…Ø§Ù†Ú¯ÛŒÚº (CLARIFY: <question> Ø³Û’ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚº)
3. Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙØ±Ø§ÛÙ… Ú©Ø±ÛŒÚº (INFO: <response> Ø³Û’ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚº)
"""

        self.conversation_history.append({
            "role": "user",
            "content": user_input
        })

        response = self.llm.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                *self.conversation_history
            ]
        )

        assistant_message = response.choices[0].message.content
        self.conversation_history.append({
            "role": "assistant",
            "content": assistant_message
        })

        return self._parse_response(assistant_message)

    def _parse_response(self, response):
        """LLM Ø¬ÙˆØ§Ø¨ Ù¾Ø§Ø±Ø³ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§ÛŒÚ©Ø´Ù† Ù„ÛŒÚº"""
        if response.startswith("ACTION:"):
            command = response.replace("ACTION:", "").strip()
            self.vla.execute_command(command)
            return {"type": "action", "command": command}

        elif response.startswith("CLARIFY:"):
            question = response.replace("CLARIFY:", "").strip()
            self.vla._speak(question)
            return {"type": "clarify", "question": question}

        else:
            info = response.replace("INFO:", "").strip()
            self.vla._speak(info)
            return {"type": "info", "response": info}

    def _analyze_scene(self, image):
        """Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Û’ Ù„ÛŒÛ’ Ù…ÙˆØ¬ÙˆØ¯Û Ø³ÛŒÙ† Ú©Ø§ ØªØ¬Ø²ÛŒÛ Ú©Ø±ÛŒÚº"""
        # Ø§Ø´ÛŒØ§Ø¡ Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÙˆÛŒÚ˜Ù† Ù…Ø§ÚˆÙ„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
        detections = self.vla.vla_model.detect_objects(image)

        return {
            "objects": [d["label"] for d in detections],
            "robot_position": self.vla.robot.get_position(),
            "gripper_state": self.vla.robot.get_gripper_state()
        }


# Ú¯ÙØªÚ¯Ùˆ Ú©ÛŒ Ù…Ø«Ø§Ù„:
# ØµØ§Ø±Ù: "Ø§Ø±Û’ Ø±ÙˆØ¨ÙˆÙ¹ØŒ Ú©ÛŒØ§ ØªÙ… ØµÙØ§Ø¦ÛŒ Ù…ÛŒÚº Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªÛ’ ÛÙˆØŸ"
# Ø±ÙˆØ¨ÙˆÙ¹: "Ø¶Ø±ÙˆØ±! Ù…Ø¬Ú¾Û’ Ø§ÛŒÚ© Ø³ÛŒØ¨ØŒ Ø§ÛŒÚ© Ú©Ù¾ Ø§ÙˆØ± Ú©Ú†Ú¾ Ú©Ø§ØºØ°Ø§Øª Ù†Ø¸Ø± Ø¢ Ø±ÛÛ’ ÛÛŒÚºÛ” Ú©ÛØ§Úº Ø³Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±ÙˆÚºØŸ"
# ØµØ§Ø±Ù: "Ø³ÛŒØ¨ Ú©Ùˆ Ù¾ÛŒØ§Ù„Û’ Ù…ÛŒÚº Ø±Ú©Ú¾Ùˆ"
# Ø±ÙˆØ¨ÙˆÙ¹: "Ø³ÛŒØ¨ Ø§Ù¹Ú¾Ø§ Ø±ÛØ§ ÛÙˆÚº... Ù¾ÛŒØ§Ù„Û’ Ù…ÛŒÚº Ø±Ú©Ú¾ Ø±ÛØ§ ÛÙˆÚº... ÛÙˆ Ú¯ÛŒØ§!"
# ØµØ§Ø±Ù: "Ø§Ø¨ Ú©Ù¾"
# Ø±ÙˆØ¨ÙˆÙ¹: "Ù…Ø¬Ú¾Û’ Ø¯Ùˆ Ú©Ù¾ Ù†Ø¸Ø± Ø¢ Ø±ÛÛ’ ÛÛŒÚº - Ø³Ø±Ø® ÙˆØ§Ù„Ø§ ÛŒØ§ Ù†ÛŒÙ„Ø§ØŸ"
# ØµØ§Ø±Ù: "Ø³Ø±Ø® ÙˆØ§Ù„Ø§"
# Ø±ÙˆØ¨ÙˆÙ¹: "Ù¹Ú¾ÛŒÚ© ÛÛ’ØŒ Ø³Ø±Ø® Ú©Ù¾ Ù„Û’ Ø±ÛØ§ ÛÙˆÚº..."
```

### Ø¢ÙˆØ§Ø² Ú©Ù…Ø§Ù†Úˆ Ø³Ù¹ÛŒÙ¹ Ù…Ø´ÛŒÙ†

Ø¢ÙˆØ§Ø² Ú©ÛŒ ØªØ¹Ø§Ù…Ù„Ø§Øª Ú©Û’ Ù„ÛŒÛ’ Ù…Ø¶Ø¨ÙˆØ· Ø§Ø³Ù¹ÛŒÙ¹ ÛÛŒÙ†ÚˆÙ„Ù†Ú¯ Ù„Ø§Ú¯Ùˆ Ú©Ø±ÛŒÚº:

```python
from enum import Enum, auto

class VoiceState(Enum):
    IDLE = auto()           # ÙˆÛŒÚ© ÙˆØ±Úˆ Ú©Ø§ Ø§Ù†ØªØ¸Ø§Ø±
    LISTENING = auto()      # Ú©Ù…Ø§Ù†Úˆ Ø±ÛŒÚ©Ø§Ø±Úˆ Ú©Ø±Ù†Ø§
    PROCESSING = auto()     # Ú©Ù…Ø§Ù†Úˆ Ø³Ù…Ø¬Ú¾Ù†Ø§
    CONFIRMING = auto()     # ØªØµØ¯ÛŒÙ‚ Ú©Ø§ Ø§Ù†ØªØ¸Ø§Ø±
    EXECUTING = auto()      # Ú©Ø§Ù… Ú†Ù„ Ø±ÛØ§ ÛÛ’
    ERROR = auto()          # ØºÙ„Ø·ÛŒ Ø³Ù†Ø¨Ú¾Ø§Ù„Ù†Ø§

class VoiceStateMachine:
    """Ø¢ÙˆØ§Ø² Ø³Û’ Ú©Ù†Ù¹Ø±ÙˆÙ„ Ø±ÙˆØ¨ÙˆÙ¹ Ú©Û’ Ù„ÛŒÛ’ Ø³Ù¹ÛŒÙ¹ Ù…Ø´ÛŒÙ†"""

    def __init__(self, vla_system):
        self.vla = vla_system
        self.state = VoiceState.IDLE
        self.pending_command = None
        self.retry_count = 0
        self.max_retries = 3

    def transition(self, event, data=None):
        """Ø³Ù¹ÛŒÙ¹ Ù¹Ø±Ø§Ù†Ø²ÛŒØ´Ù† Ø³Ù†Ø¨Ú¾Ø§Ù„ÛŒÚº"""
        transitions = {
            VoiceState.IDLE: {
                'wake_word': (VoiceState.LISTENING, self._start_listening),
            },
            VoiceState.LISTENING: {
                'speech_end': (VoiceState.PROCESSING, self._process_speech),
                'timeout': (VoiceState.IDLE, self._timeout_message),
            },
            VoiceState.PROCESSING: {
                'understood': (VoiceState.CONFIRMING, self._confirm_command),
                'unclear': (VoiceState.LISTENING, self._ask_repeat),
                'error': (VoiceState.ERROR, self._handle_error),
            },
            VoiceState.CONFIRMING: {
                'confirmed': (VoiceState.EXECUTING, self._execute_command),
                'denied': (VoiceState.IDLE, self._cancel_command),
                'modify': (VoiceState.LISTENING, self._modify_command),
            },
            VoiceState.EXECUTING: {
                'complete': (VoiceState.IDLE, self._task_complete),
                'failed': (VoiceState.ERROR, self._handle_failure),
                'stop': (VoiceState.IDLE, self._emergency_stop),
            },
            VoiceState.ERROR: {
                'retry': (VoiceState.LISTENING, self._retry),
                'abort': (VoiceState.IDLE, self._abort),
            },
        }

        if event in transitions.get(self.state, {}):
            new_state, action = transitions[self.state][event]
            print(f"Ø³Ù¹ÛŒÙ¹: {self.state.name} -> {new_state.name}")
            self.state = new_state
            action(data)
        else:
            print(f"ØºÙ„Ø· Ù¹Ø±Ø§Ù†Ø²ÛŒØ´Ù†: {self.state.name} + {event}")

    def _start_listening(self, _):
        self.vla._speak("Ø¬ÛŒØŸ")

    def _process_speech(self, audio):
        text = self.vla._process_voice(audio)
        if text and len(text) > 2:
            self.pending_command = text
            self.transition('understood', text)
        else:
            self.transition('unclear')

    def _confirm_command(self, command):
        self.vla._speak(f"Ú©ÛŒØ§ Ù…ÛŒÚº {command} Ú©Ø±ÙˆÚºØŸ")

    def _execute_command(self, _):
        self.vla._speak("Ø§Ø¨Ú¾ÛŒ Ø´Ø±ÙˆØ¹ Ú©Ø±ØªØ§ ÛÙˆÚº")
        self.vla.current_task = self.pending_command
        self.vla.task_active = True

    def _task_complete(self, _):
        self.vla._speak("ÛÙˆ Ú¯ÛŒØ§!")
        self.pending_command = None

    def _ask_repeat(self, _):
        self.retry_count += 1
        if self.retry_count >= self.max_retries:
            self.transition('abort')
        else:
            self.vla._speak("Ù…Ø¹Ø°Ø±ØªØŒ Ø¯ÙˆØ¨Ø§Ø±Û Ú©ÛÛŒÚºØŸ")

    def _emergency_stop(self, _):
        self.vla.robot.stop()
        self.vla.task_active = False
        self.vla._speak("Ø±Ú© Ú¯ÛŒØ§")

    def _handle_error(self, error):
        self.vla._speak(f"ØºÙ„Ø·ÛŒ: {error}")

    def _handle_failure(self, _):
        self.vla._speak("Ú©Ø§Ù… Ù†Ø§Ú©Ø§Ù…Û” Ø¯ÙˆØ¨Ø§Ø±Û Ú©ÙˆØ´Ø´ Ú©Ø±ÙˆÚºØŸ")

    def _retry(self, _):
        self.retry_count = 0

    def _abort(self, _):
        self.vla._speak("Ù¹Ú¾ÛŒÚ© ÛÛ’ØŒ Ø¬Ø¨ Ø¶Ø±ÙˆØ±Øª ÛÙˆ Ø¨ØªØ§Ø¦ÛŒÚº")
        self.pending_command = None
        self.retry_count = 0

    def _cancel_command(self, _):
        self.vla._speak("Ù…Ù†Ø³ÙˆØ®")
        self.pending_command = None

    def _modify_command(self, _):
        self.vla._speak("Ú©ÛŒØ§ Ú©Ø±ÙˆÚº Ø¨Ø¯Ù„Û’ Ù…ÛŒÚºØŸ")

    def _timeout_message(self, _):
        self.vla._speak("Ú©Ú†Ú¾ Ù†ÛÛŒÚº Ø³Ù†Ø§")
```

### Ú©Ø«ÛŒØ± Ø²Ø¨Ø§Ù† Ø¢ÙˆØ§Ø² Ø³Ù¾ÙˆØ±Ù¹

Ø§Ø±Ø¯Ùˆ Ø³Ù…ÛŒØª Ù…ØªØ¹Ø¯Ø¯ Ø²Ø¨Ø§Ù†ÙˆÚº Ú©ÛŒ Ø³Ù¾ÙˆØ±Ù¹:

```python
class MultiLanguageVoice:
    """VLA Ú©Û’ Ù„ÛŒÛ’ Ú©Ø«ÛŒØ± Ø²Ø¨Ø§Ù† Ø¢ÙˆØ§Ø² Ø³Ù¾ÙˆØ±Ù¹"""

    SUPPORTED_LANGUAGES = {
        'en': {'name': 'English', 'tts_lang': 'en'},
        'ur': {'name': 'Ø§Ø±Ø¯Ùˆ', 'tts_lang': 'ur'},
        'hi': {'name': 'à¤¹à¤¿à¤‚à¤¦à¥€', 'tts_lang': 'hi'},
        'ar': {'name': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©', 'tts_lang': 'ar'},
    }

    def __init__(self, default_lang='ur'):
        self.whisper = whisper.load_model("medium")  # Ø¨ÛØªØ± Ú©Ø«ÛŒØ± Ø²Ø¨Ø§Ù†
        self.current_lang = default_lang

    def transcribe(self, audio):
        """Ø²Ø¨Ø§Ù† Ú©ÛŒ Ø´Ù†Ø§Ø®Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ø¢ÚˆÛŒÙˆ Ù¹Ø±Ø§Ù†Ø³Ú©Ø±Ø§Ø¦Ø¨ Ú©Ø±ÛŒÚº"""
        result = self.whisper.transcribe(
            audio,
            task="transcribe"
        )

        detected_lang = result.get("language", "ur")
        text = result["text"].strip()

        return {
            "text": text,
            "language": detected_lang,
            "confidence": result.get("confidence", 0)
        }

    def speak(self, text, lang=None):
        """Ù…Ø®ØµÙˆØµ Ø²Ø¨Ø§Ù† Ù…ÛŒÚº Ù¹ÛŒÚ©Ø³Ù¹ Ø³Û’ Ø¢ÙˆØ§Ø²"""
        lang = lang or self.current_lang
        tts_lang = self.SUPPORTED_LANGUAGES.get(lang, {}).get('tts_lang', 'ur')

        tts = gTTS(text=text, lang=tts_lang)
        tts.save("/tmp/response.mp3")
        self._play_audio("/tmp/response.mp3")

    def translate_command(self, text, source_lang, target_lang='en'):
        """Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ú©Û’ Ù„ÛŒÛ’ Ú©Ù…Ø§Ù†Úˆ Ú©Ùˆ Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ù…ÛŒÚº ØªØ±Ø¬Ù…Û Ú©Ø±ÛŒÚº"""
        if source_lang == target_lang:
            return text

        # ØªØ±Ø¬Ù…Û API ÛŒØ§ Ù…Ø§ÚˆÙ„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
        # Ø§Ø³ Ø³Û’ ÛŒÙ‚ÛŒÙ†ÛŒ ÛÙˆØªØ§ ÛÛ’ Ú©Û VLA Ù…Ø§ÚˆÙ„ Ú©Ùˆ Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ú©Ù…Ø§Ù†ÚˆØ² Ù…Ù„ÛŒÚº
        translated = self._translate(text, source_lang, target_lang)
        return translated

    def _translate(self, text, src, tgt):
        """ØªØ±Ø¬Ù…Û Ú©Ø§ Ù†ÙØ§Ø°"""
        # OpenAI ÛŒØ§ Ù…Ø®ØµÙˆØµ ØªØ±Ø¬Ù…Û Ø³Ø±ÙˆØ³ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
        from openai import OpenAI
        client = OpenAI()

        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": f"{src} Ø³Û’ {tgt} Ù…ÛŒÚº ØªØ±Ø¬Ù…Û Ú©Ø±ÛŒÚºÛ” ØµØ±Ù ØªØ±Ø¬Ù…Û Ù„Ú©Ú¾ÛŒÚºÛ”"},
                {"role": "user", "content": text}
            ]
        )
        return response.choices[0].message.content


# Ø§Ø±Ø¯Ùˆ Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ø³ØªØ¹Ù…Ø§Ù„
voice = MultiLanguageVoice(default_lang='ur')
result = voice.transcribe(audio)
# result: {"text": "Ø³ÛŒØ¨ Ø§Ù¹Ú¾Ø§Ø¤", "language": "ur"}

english_command = voice.translate_command(result["text"], "ur", "en")
# english_command: "pick up the apple"

# Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒÚº Ø§ÙˆØ± Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚº
vla.execute(english_command)
voice.speak("Ø³ÛŒØ¨ Ø§Ù¹Ú¾Ø§ Ù„ÛŒØ§", lang='ur')  # "Picked up the apple" Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº
```

---

## Ø¹Ù…Ù„ÛŒ Ù„ÛŒØ¨

### Ù„ÛŒØ¨ 4.5A: VLA Ø³Ø³Ù¹Ù… ÚˆÛŒÙ¾Ù„Ø§Ø¦ÛŒ Ú©Ø±ÛŒÚº

Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ VLA ÚˆÛŒÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ø¨Ù†Ø§Ø¦ÛŒÚº Ø¬Ø³ Ù…ÛŒÚº:
1. Ù…Ù„Ù¹ÛŒ Ú©ÛŒÙ…Ø±Û Ø§Ù† Ù¾Ù¹
2. Ø­ÙØ§Ø¸ØªÛŒ ÙÙ„Ù¹Ø±Ù†Ú¯
3. Ù†Ø§Ú©Ø§Ù…ÛŒ Ú©ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
4. Ø±ÛŒØ¦Ù„ Ù¹Ø§Ø¦Ù… Ù¾Ø±ÙØ§Ø±Ù…Ù†Ø³

### Ù„ÛŒØ¨ 4.5B: Ø¢ÙˆØ§Ø² Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Ø§ VLA Ø±ÙˆØ¨ÙˆÙ¹

Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ Ø¢ÙˆØ§Ø² Ø³Û’ Ú©Ù†Ù¹Ø±ÙˆÙ„ VLA Ø³Ø³Ù¹Ù… Ø¨Ù†Ø§Ø¦ÛŒÚº:

```python
# lab_voice_vla.py
"""
Ù„ÛŒØ¨ 4.5B: Ø¢ÙˆØ§Ø² Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Ø§ VLA Ø±ÙˆØ¨ÙˆÙ¹
VLA Ø§ÙˆØ± Whisper Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø¢ÙˆØ§Ø² Ø³Û’ Ú©Ù†Ù¹Ø±ÙˆÙ„ Ø±ÙˆØ¨ÙˆÙ¹ Ø¨Ù†Ø§Ø¦ÛŒÚº
"""

# Ù…Ø±Ø­Ù„Û 1: Ø§Ù†Ø­ØµØ§Ø±Ø§Øª Ø§Ù†Ø³Ù¹Ø§Ù„ Ú©Ø±ÛŒÚº
# pip install openai-whisper sounddevice numpy gtts pygame transformers

# Ù…Ø±Ø­Ù„Û 2: VoiceVLALab Ú©Ù„Ø§Ø³ Ø¨Ù†Ø§Ø¦ÛŒÚº
class VoiceVLALab:
    """Ø¢ÙˆØ§Ø² Ø³Û’ Ú©Ù†Ù¹Ø±ÙˆÙ„ VLA Ú©Ø§ Ù„ÛŒØ¨ Ù†ÙØ§Ø°"""

    def __init__(self):
        import whisper
        from gtts import gTTS

        # Whisper Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº
        print("Whisper Ù…Ø§ÚˆÙ„ Ù„ÙˆÚˆ ÛÙˆ Ø±ÛØ§ ÛÛ’...")
        self.whisper = whisper.load_model("base")

        # Ø³Ù…ÛŒÙˆÙ„ÛŒÙ¹Úˆ Ø±ÙˆØ¨ÙˆÙ¹ Ø³Ù¹ÛŒÙ¹
        self.robot_state = {
            'position': [0, 0, 0],
            'gripper': 'open',
            'current_task': None
        }

        # Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ ÚˆÛŒÙ¹Ø§Ø¨ÛŒØ³ (Ø³ÛŒÙ† Ø³Ù…ÛŒÙˆÙ„ÛŒÙ¹)
        self.scene_objects = [
            {'name': 'Ø³ÛŒØ¨', 'color': 'Ø³Ø±Ø®', 'position': [0.3, 0, 0]},
            {'name': 'Ù¾ÛŒØ§Ù„Û', 'color': 'Ù†ÛŒÙ„Ø§', 'position': [0.4, 0.2, 0]},
            {'name': 'Ú©Ù¾', 'color': 'Ø³ÙÛŒØ¯', 'position': [0.2, -0.2, 0]},
        ]

    def listen_and_execute(self):
        """Ù…ÛŒÙ† ÚˆÛŒÙ…Ùˆ Ù„ÙˆÙ¾"""
        import sounddevice as sd
        import numpy as np

        print("\n" + "="*50)
        print("Ø¢ÙˆØ§Ø² Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Ø§ VLA Ø±ÙˆØ¨ÙˆÙ¹ Ù„ÛŒØ¨")
        print("="*50)
        print("Ú©Ù…Ø§Ù†ÚˆØ²: '[Ú†ÛŒØ²] Ø§Ù¹Ú¾Ø§Ø¤', '[Ú†ÛŒØ²] Ú©Ùˆ [Ø¬Ú¯Û] Ù…ÛŒÚº Ø±Ú©Ú¾Ùˆ'")
        print("        'Ú©ÛŒØ§ Ù†Ø¸Ø± Ø¢ØªØ§ ÛÛ’', 'Ø±Ú©Ùˆ', 'Ø¨Ø§ÛØ±'")
        print("="*50 + "\n")

        while True:
            # Ø¢ÚˆÛŒÙˆ Ø±ÛŒÚ©Ø§Ø±Úˆ Ú©Ø±ÛŒÚº
            print("Ø³Ù† Ø±ÛØ§ ÛÙˆÚº... (4 Ø³ÛŒÚ©Ù†Úˆ Ø¨ÙˆÙ„ÛŒÚº)")
            audio = sd.rec(int(4 * 16000), samplerate=16000,
                          channels=1, dtype=np.int16)
            sd.wait()

            # Ù¹Ø±Ø§Ù†Ø³Ú©Ø±Ø§Ø¦Ø¨ Ú©Ø±ÛŒÚº
            audio_float = audio.flatten().astype(np.float32) / 32768.0
            result = self.whisper.transcribe(audio_float, language="ur")
            command = result["text"].strip()

            print(f"Ø¢Ù¾ Ù†Û’ Ú©ÛØ§: {command}")

            if not command:
                continue

            # Ú©Ù…Ø§Ù†Úˆ Ù¾Ø±ÙˆØ³ÛŒØ³ Ú©Ø±ÛŒÚº
            if 'Ø¨Ø§ÛØ±' in command.lower() or 'Ø®Ø§Ø±Ø¬' in command.lower():
                self.speak("Ø®Ø¯Ø§ Ø­Ø§ÙØ¸!")
                break

            self.process_command(command)

    def process_command(self, command):
        """Ø¢ÙˆØ§Ø² Ú©Ù…Ø§Ù†Úˆ Ù¾Ø±ÙˆØ³ÛŒØ³ Ú©Ø±ÛŒÚº"""
        command_lower = command.lower()

        # Ø³ÛŒÙ† Ú©Ø§ Ø³ÙˆØ§Ù„
        if 'Ú©ÛŒØ§ Ù†Ø¸Ø±' in command_lower or 'Ú©ÛŒØ§ Ø¯Ú©Ú¾ØªØ§' in command_lower:
            objects = [f"{o['color']} {o['name']}" for o in self.scene_objects]
            response = f"Ù…Ø¬Ú¾Û’ Ù†Ø¸Ø± Ø¢ Ø±ÛØ§ ÛÛ’: {', '.join(objects)}"
            print(f"Ø±ÙˆØ¨ÙˆÙ¹: {response}")
            self.speak(response)
            return

        # Ø±ÙˆÚ©Ù†Û’ Ú©ÛŒ Ú©Ù…Ø§Ù†Úˆ
        if 'Ø±Ú©Ùˆ' in command_lower or 'Ø¨Ù†Ø¯' in command_lower:
            print("Ø±ÙˆØ¨ÙˆÙ¹: ØªÙ…Ø§Ù… Ø§ÛŒÚ©Ø´Ù†Ø² Ø±Ú© Ø±ÛÛ’ ÛÛŒÚº")
            self.speak("Ø±Ú© Ø±ÛØ§ ÛÙˆÚº")
            self.robot_state['current_task'] = None
            return

        # Ø§Ù¹Ú¾Ø§Ù†Û’ Ú©ÛŒ Ú©Ù…Ø§Ù†Úˆ
        if 'Ø§Ù¹Ú¾Ø§' in command_lower:
            for obj in self.scene_objects:
                if obj['name'] in command_lower or obj['color'] in command_lower:
                    self.simulate_pick(obj)
                    return
            self.speak("ÙˆÛ Ú†ÛŒØ² Ù†ÛÛŒÚº Ù…Ù„ÛŒ")
            return

        # Ø±Ú©Ú¾Ù†Û’ Ú©ÛŒ Ú©Ù…Ø§Ù†Úˆ
        if 'Ø±Ú©Ú¾' in command_lower:
            # Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ Ø§ÙˆØ± Ù…Ù†Ø²Ù„ ØªÙ„Ø§Ø´ Ú©Ø±ÛŒÚº
            held_obj = self.robot_state.get('holding')
            if held_obj:
                for obj in self.scene_objects:
                    if obj['name'] in command_lower:
                        self.simulate_place(held_obj, obj)
                        return
            self.speak("Ù…ÛŒØ±Û’ ÛØ§ØªÚ¾ Ù…ÛŒÚº Ú©Ú†Ú¾ Ù†ÛÛŒÚº")
            return

        # Ù†Ø§Ù…Ø¹Ù„ÙˆÙ… Ú©Ù…Ø§Ù†Úˆ
        self.speak("ÛŒÛ Ú©Ù…Ø§Ù†Úˆ Ø³Ù…Ø¬Ú¾ Ù†ÛÛŒÚº Ø¢Ø¦ÛŒ")

    def simulate_pick(self, obj):
        """Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ Ø§Ù¹Ú¾Ø§Ù†Û’ Ú©ÛŒ Ø³Ù…ÛŒÙˆÙ„ÛŒØ´Ù†"""
        print(f"Ø±ÙˆØ¨ÙˆÙ¹: {obj['name']} Ú©ÛŒ Ø·Ø±Ù Ø¬Ø§ Ø±ÛØ§ ÛÙˆÚº...")
        self.speak(f"{obj['color']} {obj['name']} Ø§Ù¹Ú¾Ø§ Ø±ÛØ§ ÛÙˆÚº")

        # Ø­Ø±Ú©Øª Ú©ÛŒ Ø³Ù…ÛŒÙˆÙ„ÛŒØ´Ù†
        import time
        for i in range(3):
            print(f"  Ù…Ø±Ø­Ù„Û {i+1}: Ù‚Ø±ÛŒØ¨ Ø¢ Ø±ÛØ§ ÛÙˆÚº...")
            time.sleep(0.5)

        print(f"  Ú¯Ø±Ù¾Ø± {obj['name']} Ù¾Ø± Ø¨Ù†Ø¯ ÛÙˆ Ø±ÛØ§ ÛÛ’")
        self.robot_state['gripper'] = 'closed'
        self.robot_state['holding'] = obj

        print(f"Ø±ÙˆØ¨ÙˆÙ¹: {obj['name']} Ø§Ù¹Ú¾Ø§ Ù„ÛŒØ§!")
        self.speak("Ù„Û’ Ù„ÛŒØ§!")

    def simulate_place(self, obj, destination):
        """Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ Ø±Ú©Ú¾Ù†Û’ Ú©ÛŒ Ø³Ù…ÛŒÙˆÙ„ÛŒØ´Ù†"""
        print(f"Ø±ÙˆØ¨ÙˆÙ¹: {destination['name']} Ú©ÛŒ Ø·Ø±Ù Ø¬Ø§ Ø±ÛØ§ ÛÙˆÚº...")
        self.speak(f"{destination['name']} Ù…ÛŒÚº Ø±Ú©Ú¾ Ø±ÛØ§ ÛÙˆÚº")

        import time
        for i in range(3):
            print(f"  Ù…Ø±Ø­Ù„Û {i+1}: Ø­Ø±Ú©Øª...")
            time.sleep(0.5)

        print(f"  Ú¯Ø±Ù¾Ø± Ú©Ú¾Ù„ Ø±ÛØ§ ÛÛ’")
        self.robot_state['gripper'] = 'open'
        self.robot_state['holding'] = None

        print(f"Ø±ÙˆØ¨ÙˆÙ¹: {obj['name']} {destination['name']} Ù…ÛŒÚº Ø±Ú©Ú¾ Ø¯ÛŒØ§!")
        self.speak("ÛÙˆ Ú¯ÛŒØ§!")

    def speak(self, text):
        """Ù¹ÛŒÚ©Ø³Ù¹ Ø³Û’ Ø¢ÙˆØ§Ø²"""
        try:
            from gtts import gTTS
            import pygame
            import io

            tts = gTTS(text=text, lang='ur')
            fp = io.BytesIO()
            tts.write_to_fp(fp)
            fp.seek(0)

            pygame.mixer.init()
            pygame.mixer.music.load(fp)
            pygame.mixer.music.play()
            while pygame.mixer.music.get_busy():
                pygame.time.wait(100)
        except Exception as e:
            print(f"[TTS ØºÙ„Ø·ÛŒ: {e}]")


# Ù„ÛŒØ¨ Ú†Ù„Ø§Ø¦ÛŒÚº
if __name__ == "__main__":
    lab = VoiceVLALab()
    lab.listen_and_execute()
```

**Ù„ÛŒØ¨ Ú©Û’ Ù…Ù‚Ø§ØµØ¯:**
1. Whisper Ø§Ø³Ù¾ÛŒÚ† Ø±ÛŒÚ©Ú¯Ù†ÛŒØ´Ù† Ú©Ùˆ VLA Ú©Û’ Ø³Ø§ØªÚ¾ ÛŒÚ©Ø¬Ø§ Ú©Ø±ÛŒÚº
2. Ø¢ÙˆØ§Ø² Ú©Ù…Ø§Ù†Úˆ Ù¾Ø§Ø±Ø³Ù†Ú¯ Ù„Ø§Ú¯Ùˆ Ú©Ø±ÛŒÚº
3. Ù¹ÛŒÚ©Ø³Ù¹ Ø³Û’ Ø¢ÙˆØ§Ø² ÙÛŒÚˆØ¨ÛŒÚ© Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº
4. Ù…Ù„Ù¹ÛŒ Ø³Ù¹ÛŒÙ¾ Ø¢ÙˆØ§Ø² ØªØ¹Ø§Ù…Ù„Ø§Øª Ø³Ù†Ø¨Ú¾Ø§Ù„ÛŒÚº
5. Ø³ÛŒÙ† Ú©Û’ Ø³ÙˆØ§Ù„Ø§Øª Ú©ÛŒ Ø³Ù¾ÙˆØ±Ù¹ ("Ú©ÛŒØ§ Ù†Ø¸Ø± Ø¢ØªØ§ ÛÛ’ØŸ")

**Ù…ØªÙˆÙ‚Ø¹ ÚˆÛŒÙ…Ùˆ ÙÙ„Ùˆ:**
```
Ø¢ÙˆØ§Ø² Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Ø§ VLA Ø±ÙˆØ¨ÙˆÙ¹ Ù„ÛŒØ¨
==================================================
Ú©Ù…Ø§Ù†ÚˆØ²: '[Ú†ÛŒØ²] Ø§Ù¹Ú¾Ø§Ø¤', '[Ú†ÛŒØ²] Ú©Ùˆ [Ø¬Ú¯Û] Ù…ÛŒÚº Ø±Ú©Ú¾Ùˆ'
        'Ú©ÛŒØ§ Ù†Ø¸Ø± Ø¢ØªØ§ ÛÛ’', 'Ø±Ú©Ùˆ', 'Ø¨Ø§ÛØ±'
==================================================

Ø³Ù† Ø±ÛØ§ ÛÙˆÚº... (4 Ø³ÛŒÚ©Ù†Úˆ Ø¨ÙˆÙ„ÛŒÚº)
Ø¢Ù¾ Ù†Û’ Ú©ÛØ§: Ú©ÛŒØ§ Ù†Ø¸Ø± Ø¢ØªØ§ ÛÛ’ØŸ
Ø±ÙˆØ¨ÙˆÙ¹: Ù…Ø¬Ú¾Û’ Ù†Ø¸Ø± Ø¢ Ø±ÛØ§ ÛÛ’: Ø³Ø±Ø® Ø³ÛŒØ¨ØŒ Ù†ÛŒÙ„Ø§ Ù¾ÛŒØ§Ù„ÛØŒ Ø³ÙÛŒØ¯ Ú©Ù¾

Ø³Ù† Ø±ÛØ§ ÛÙˆÚº... (4 Ø³ÛŒÚ©Ù†Úˆ Ø¨ÙˆÙ„ÛŒÚº)
Ø¢Ù¾ Ù†Û’ Ú©ÛØ§: Ø³Ø±Ø® Ø³ÛŒØ¨ Ø§Ù¹Ú¾Ø§Ø¤
Ø±ÙˆØ¨ÙˆÙ¹: Ø³ÛŒØ¨ Ú©ÛŒ Ø·Ø±Ù Ø¬Ø§ Ø±ÛØ§ ÛÙˆÚº...
  Ù…Ø±Ø­Ù„Û 1: Ù‚Ø±ÛŒØ¨ Ø¢ Ø±ÛØ§ ÛÙˆÚº...
  Ù…Ø±Ø­Ù„Û 2: Ù‚Ø±ÛŒØ¨ Ø¢ Ø±ÛØ§ ÛÙˆÚº...
  Ù…Ø±Ø­Ù„Û 3: Ù‚Ø±ÛŒØ¨ Ø¢ Ø±ÛØ§ ÛÙˆÚº...
  Ú¯Ø±Ù¾Ø± Ø³ÛŒØ¨ Ù¾Ø± Ø¨Ù†Ø¯ ÛÙˆ Ø±ÛØ§ ÛÛ’
Ø±ÙˆØ¨ÙˆÙ¹: Ø³ÛŒØ¨ Ø§Ù¹Ú¾Ø§ Ù„ÛŒØ§!

Ø³Ù† Ø±ÛØ§ ÛÙˆÚº... (4 Ø³ÛŒÚ©Ù†Úˆ Ø¨ÙˆÙ„ÛŒÚº)
Ø¢Ù¾ Ù†Û’ Ú©ÛØ§: Ù¾ÛŒØ§Ù„Û’ Ù…ÛŒÚº Ø±Ú©Ú¾Ùˆ
Ø±ÙˆØ¨ÙˆÙ¹: Ù¾ÛŒØ§Ù„Û’ Ú©ÛŒ Ø·Ø±Ù Ø¬Ø§ Ø±ÛØ§ ÛÙˆÚº...
  Ù…Ø±Ø­Ù„Û 1: Ø­Ø±Ú©Øª...
  Ù…Ø±Ø­Ù„Û 2: Ø­Ø±Ú©Øª...
  Ù…Ø±Ø­Ù„Û 3: Ø­Ø±Ú©Øª...
  Ú¯Ø±Ù¾Ø± Ú©Ú¾Ù„ Ø±ÛØ§ ÛÛ’
Ø±ÙˆØ¨ÙˆÙ¹: Ø³ÛŒØ¨ Ù¾ÛŒØ§Ù„Û’ Ù…ÛŒÚº Ø±Ú©Ú¾ Ø¯ÛŒØ§!
```

## Ø®Ù„Ø§ØµÛ

- VLA Ø³Ø³Ù¹Ù…Ø² ØªÙ…Ø§Ù… Ø§Ø¬Ø²Ø§Ø¡ Ú©Ùˆ Ø§ÛŒÙ†Úˆ Ù¹Ùˆ Ø§ÛŒÙ†Úˆ ÛŒÚ©Ø¬Ø§ Ú©Ø±ØªÛ’ ÛÛŒÚº
- Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©ÛŒ ÚˆÛŒÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ù„ÛŒÙ¹Ù†Ø³ÛŒ Ø§ÙˆØ± Ø­ÙØ§Ø¸Øª Ú©ÛŒ ÛÛŒÙ†ÚˆÙ„Ù†Ú¯ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’
- Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ù¾Ø± ÚˆÛŒÙ¾Ù„Ø§Ø¦Ù…Ù†Ù¹ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªÛŒ ÛÛ’
- **Ø¢ÙˆØ§Ø² Ú©Ù†Ù¹Ø±ÙˆÙ„ ÛÛŒÙ†ÚˆØ² ÙØ±ÛŒ Ø±ÙˆØ¨ÙˆÙ¹ Ø¢Ù¾Ø±ÛŒØ´Ù† Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªØ§ ÛÛ’**
- **ÙˆÛŒÚ© ÙˆØ±ÚˆØ² Ø§ÙˆØ± Ø³Ù¹ÛŒÙ¹ Ù…Ø´ÛŒÙ†Ø² Ù…Ø¶Ø¨ÙˆØ· Ø¢ÙˆØ§Ø² Ø§Ù†Ù¹Ø±ÙÛŒØ³Ø² Ø¨Ù†Ø§ØªÛŒ ÛÛŒÚº**
- **Ú¯ÙØªÚ¯Ùˆ AI Ù‚Ø¯Ø±ØªÛŒ Ù…Ù„Ù¹ÛŒ Ù¹Ø±Ù† ØªØ¹Ø§Ù…Ù„Ø§Øª Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªÛŒ ÛÛ’**
- **Ú©Ø«ÛŒØ± Ø²Ø¨Ø§Ù† Ø³Ù¾ÙˆØ±Ù¹ Ø±Ø³Ø§Ø¦ÛŒ Ø¨Ú‘Ú¾Ø§ØªÛŒ ÛÛ’ (Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒØŒ Ø§Ø±Ø¯ÙˆØŒ ÙˆØºÛŒØ±Û)**

## Ù…Ø²ÛŒØ¯ Ù¾Ú‘Ú¾Ø§Ø¦ÛŒ

- [OpenAI Whisper](https://github.com/openai/whisper) - Ø§Ø³Ù¾ÛŒÚ† Ø±ÛŒÚ©Ú¯Ù†ÛŒØ´Ù†
- [Porcupine](https://picovoice.ai/platform/porcupine/) - ÙˆÛŒÚ© ÙˆØ±Úˆ ÚˆÛŒÙ¹ÛŒÚ©Ø´Ù†
- [OpenVLA](https://openvla.github.io/) - ÙˆÛŒÚ˜Ù†-Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬-Ø§ÛŒÚ©Ø´Ù† Ù…Ø§ÚˆÙ„Ø²
- [ROS 2 Audio](https://github.com/ros-drivers/audio_common) - Ø¢ÚˆÛŒÙˆ Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒØ´Ù†

[Ø¨Ø§Ø¨ 4.6: Ú©ÛŒÙ¾Ø³Ù¹ÙˆÙ† Ù¾Ø± Ø¬Ø§Ø¦ÛŒÚº â†’](/docs/module-4-vla/chapter-6-capstone)
